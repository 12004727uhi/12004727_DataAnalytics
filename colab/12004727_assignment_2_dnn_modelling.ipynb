{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12004727_assignment_2_dnn_modelling.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/12004727uhi/12004727_DataAnalytics/blob/master/colab/12004727_assignment_2_dnn_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owhs2I5VQg0o"
      },
      "source": [
        "# import pandas, to create data frame\n",
        "import pandas as pd\n",
        "# import numpy for maths operations\n",
        "import numpy as np\n",
        "# import tensorflow for modelling\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "# import file management utilities\n",
        "import shutil\n",
        "# import package to create data tables\n",
        "from tabulate import tabulate\n",
        "# import iterable tools\n",
        "import itertools\n",
        "# import time for timestamping\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDGhR6r9ZaJM"
      },
      "source": [
        "## define helper functions\n",
        "#\n",
        "# function to remove data\n",
        "def remove_predictors(TRAIN_AND_TEST_DATA, VALIDATION_DATA, data_type, remove_list):\n",
        "    def remove_temporal_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA):\n",
        "      # define predictor categories\n",
        "      month_one_hot = ['Apr','Aug','Dec','Feb','Jan','Jul','Jun','Mar','May','Nov','Oct','Sep']\n",
        "      day_of_week_one_hot = ['Fri','Mon','Sat','Sun','Thu','Tue','Wed']\n",
        "      ordinal_all= ['day_index','day_of_year','year']\n",
        "      ordinal_not_hot = ['mo', 'day']\n",
        "      temporal_columns_one_hot = month_one_hot + day_of_week_one_hot + ordinal_all\n",
        "      temporal_columns_ordinal = ordinal_not_hot + ordinal_all\n",
        "      try: # try with one-hot encoded values\n",
        "        return [TRAIN_AND_TEST_DATA.drop(columns=temporal_columns_one_hot), \n",
        "                VALIDATION_DATA.drop(columns=temporal_columns_one_hot)]\n",
        "      except: # if fails with one-hot encoded values, use month/day ordinals \n",
        "        return [TRAIN_AND_TEST_DATA.drop(columns=temporal_columns_ordinal), \n",
        "                VALIDATION_DATA.drop(columns=temporal_columns_ordinal)]\n",
        "    # funtion to remove weather data\n",
        "    def remove_weather_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA):\n",
        "      # define predictor categories\n",
        "      weather_columns = ['temp','dewp','slp','visib','wdsp','gust','max','min','prcp','sndp','fog']\n",
        "      return [TRAIN_AND_TEST_DATA.drop(columns=weather_columns), VALIDATION_DATA.drop(columns=weather_columns)]\n",
        "    # function to remove specific predictors\n",
        "    def remove_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA, remove_list):\n",
        "      if not remove_list:\n",
        "        print('\\nThere was nothing to remove!\\n')\n",
        "        return [TRAIN_AND_TEST_DATA, VALIDATION_DATA]\n",
        "      return [TRAIN_AND_TEST_DATA.drop(columns=remove_list), VALIDATION_DATA.drop(columns=remove_list)]\n",
        "    if data_type == 'TEMPORAL':\n",
        "      return remove_temporal_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA)\n",
        "    elif data_type == 'WEATHER':\n",
        "      return remove_weather_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA)\n",
        "    elif data_type == 'CUSTOM':\n",
        "      return remove_data(TRAIN_AND_TEST_DATA, VALIDATION_DATA, remove_list)\n",
        "    else:\n",
        "      return [TRAIN_AND_TEST_DATA, VALIDATION_DATA]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSlB_GOnAxO"
      },
      "source": [
        "## Configuration\n",
        "#\n",
        "# enable logging for tensorflow (set to ERROR when automating, or INFO for diagnostic)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "# define dataset friendly name (descriptive)\n",
        "DATASET_TITLE = '2013 to 2019 - One-hot Encoding [dnn_2.csv]'\n",
        "# define dataset csv name\n",
        "DATASET_NAME = 'dnn_2'\n",
        "# define dataset CSV URL\n",
        "DATA_URL = 'https://raw.githubusercontent.com/12004727uhi/12004727_DataAnalytics/master/dnn_2.csv'\n",
        "# define validation dataset CSV URL\n",
        "VALIDATION_DATA_URL = 'https://raw.githubusercontent.com/12004727uhi/12004727_DataAnalytics/master/sample_dnn_2.csv'\n",
        "# import dataset CSV & assign to variable (type pandas df)\n",
        "TRAIN_AND_TEST_DATA = pd.read_csv(DATA_URL, index_col=0,)\n",
        "# import validation dataset CSV & assign to variable (type pandas df)\n",
        "VALIDATION_DATA = pd.read_csv(VALIDATION_DATA_URL, index_col=0,)\n",
        "# remove data columns (optional)\n",
        "\"\"\"\n",
        "- to remove weather data, pass data_type 'WEATHER'\n",
        "- to remove temporal data, pass data_type 'TEMPORAL'\n",
        "- to remove custom data, pass data_type 'CUSTOM' & predictors to remove in remove_list.\n",
        "\"\"\"\n",
        "TRAIN_AND_TEST_DATA, VALIDATION_DATA = remove_predictors(\n",
        "    TRAIN_AND_TEST_DATA, VALIDATION_DATA, data_type='WEATHER', remove_list=[])\n",
        "# define number of targets\n",
        "NUM_TARGETS = 1\n",
        "# define predictors for training/testing\n",
        "PREDICTORS_COLS = [p for p in range(len(TRAIN_AND_TEST_DATA.columns)-1)]\n",
        "# define target column indexes\n",
        "TARGETS_COLS = [len(TRAIN_AND_TEST_DATA.columns)-1]\n",
        "# define validation target column name(s)\n",
        "VALIDATION_TARGETS = ['NUM_COLLISIONS']\n",
        "# URI of model (appended with timestamp to ensure unique per model)\n",
        "MODEL_URI = f'/tmp/trained_model_{DATASET_NAME}_{time.time()}'\n",
        "# define how many model runs for training & testing\n",
        "NUMBER_OF_MODEL_RUNS = 5\n",
        "# Define scale parameter for number of collisions\n",
        "SCALE_NUM_COLLISIONS = 1.0\n",
        "# define number of estimator fit steps\n",
        "ESTIMATOR_STEPS = 10000\n",
        "# define number of hidden layers\n",
        "HIDDEN_UNITS = [10, 9, 7]  # [10, 9, 7]; [20, 18, 14]; [40, 36, 28]\n",
        "# define learning rate\n",
        "LEARNING_RATE = 0.1 # [0.1; 0.01; 0.001]\n",
        "# set training set size (percentage as decimal)\n",
        "TRAINING_SET_SLICE = 0.8\n",
        "\n",
        "# show first 5 rows to verify import\n",
        "display(TRAIN_AND_TEST_DATA[:5])\n",
        "# show first 5 rows to verify import\n",
        "display(VALIDATION_DATA[:5])\n",
        "# print predictors column list to verify\n",
        "print(f'Predictors columns: {TRAIN_AND_TEST_DATA.iloc[:,PREDICTORS_COLS]}')\n",
        "# print targets columns to verify\n",
        "print(f'Target columns: {TRAIN_AND_TEST_DATA.iloc[:,TARGETS_COLS]}')\n",
        "# check the tensorflow version\n",
        "print(f'Tensor flow version: {tf.__version__}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIlOWHAA9poD"
      },
      "source": [
        "# define function to create table of training/testing results & compute RSME\n",
        "def create_results_table(rmse_results):\n",
        "  # calculate mean & median RMSEs\n",
        "  regression_RMSE_list = [n['DNN Regression Model RMSE'] for n in rmse_results]\n",
        "  mean_model_RMSE_list = [n['Mean Model RMSE'] for n in rmse_results]\n",
        "  regression_mean_RMSE = np.mean(regression_RMSE_list)\n",
        "  mean_mean_RMSE = np.mean(mean_model_RMSE_list)\n",
        "  regression_median_RMSE = np.median(regression_RMSE_list)\n",
        "  mean_median_RMSE = np.median(mean_model_RMSE_list)\n",
        "  header = rmse_results[0].keys()\n",
        "  rows =  [x.values() for x in rmse_results]\n",
        "  # ONLY PRINT OUTPUT IF regression_median_RMSE <= 1.0\n",
        "  if regression_median_RMSE < 1.0:\n",
        "    print(tabulate(rows, header, floatfmt=\".15\"))\n",
        "    # print mean & median RMSEs\n",
        "    print(f'\\nMean RMSE for \"DNN Regression Model\": {regression_mean_RMSE}')\n",
        "    print(f'Median RMSE for \"DNN Regression Model\": {regression_median_RMSE}')\n",
        "    print(f'Mean RMSE for \"Mean Model\": {mean_mean_RMSE}')\n",
        "    print(f'Median RMSE for \"Mean Model\": {mean_median_RMSE}\\n')\n",
        "    return True\n",
        "  else:\n",
        "    print(f'Median DNN Regressor RMSE was value was over 1.0! Tables will not be drawn.')\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyCcOtR0PEx"
      },
      "source": [
        "# function to create table of validation results & compute RSME\n",
        "def create_validation_table(validation_results):\n",
        "  header = validation_results[0].keys()\n",
        "  rows =  [x.values() for x in validation_results]\n",
        "  # print table\n",
        "  print(tabulate(rows, header, floatfmt=\".15\"))\n",
        "  # calculate mean & median RMSEs\n",
        "  validation_RMSE_list = [n['Validation Test RMSE'] for n in validation_results]\n",
        "  validation_mean_RMSE = np.mean(validation_RMSE_list)\n",
        "  validation_median_RMSE = np.median(validation_RMSE_list)\n",
        "  # print mean & median RMSEs\n",
        "  print(f'\\nMean RMSE for Validaton Data: {validation_mean_RMSE}')\n",
        "  print(f'Median RMSE for Validaton Data: {validation_median_RMSE}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ltk-GOJiRps"
      },
      "source": [
        "# function to generate rank table of median rmse result over n runs\n",
        "def create_rank_table(all_results):\n",
        "  # define final table lists\n",
        "  table_list = []\n",
        "  # compile table list with medians \n",
        "  for model in all_results:\n",
        "    run_rmse_list = [run['DNN Regression Model RMSE'] for run in model['model_results']]\n",
        "    run_mean_model_rmse_list = [run['Mean Model RMSE'] for run in model['model_results']]\n",
        "    run_validation_rmse_list = [run['Validation Test RMSE'] for run in model['validation_results']]\n",
        "    table_list.append({'Median\\nRMSE': np.median(run_rmse_list), \n",
        "                       'Median\\n\\'Mean Model\\'RMSE': np.median(run_mean_model_rmse_list),\n",
        "                       'Model\\nPredictors': '\\n'.join([TRAIN_AND_TEST_DATA.columns[c] for c in model['model_results'][0]['Predictors Columns']]),\n",
        "                       'Model\\nRuns': len(run_rmse_list),\n",
        "                       'Median Validation\\nRMSE': np.median(run_validation_rmse_list),\n",
        "                       'Validation\\nRuns': len(run_validation_rmse_list)\n",
        "                       })\n",
        "  # rank medians\n",
        "  sorted_table_list = sorted(table_list, key=lambda d: d['Median\\nRMSE'])\n",
        "  # display table\n",
        "  print('\\n----------------------------------------------------------------')\n",
        "  print('Results Table of Median RMSE Results for DNN Regression Models')\n",
        "  print('----------------------------------------------------------------')\n",
        "  print('Tensor Flow DNN Regression Output')\n",
        "  print(f'Model dataset title: {DATASET_TITLE}')\n",
        "  print(f'Model dataset CSV: {DATA_URL}')\n",
        "  print(f'Model validation dataset CSV: {VALIDATION_DATA_URL}')\n",
        "  print(f'Model Learning Rate: {LEARNING_RATE}')\n",
        "  print(f'Model Hidden Layers: {HIDDEN_UNITS}\\n')\n",
        "  header = sorted_table_list[0].keys()\n",
        "  rows =  [x.values() for x in sorted_table_list]\n",
        "  print(tabulate(rows, header, floatfmt=\".15\", showindex='always', tablefmt=\"grid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRirg5X5BpM_"
      },
      "source": [
        "## define function to validate against normalised, processed dataset\n",
        "# set pandas to display _all_ columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "def run_validation(validation_predictors_cols):\n",
        "  # shuffle data\n",
        "  shuffled_validation_data = VALIDATION_DATA.iloc[np.random.permutation(len(VALIDATION_DATA))]\n",
        "  # assign predictors to variables\n",
        "  predictors_validation_data = shuffled_validation_data.iloc[:,validation_predictors_cols]\n",
        "  # define TensorFlow estimator\n",
        "  estimator = tf.contrib.learn.SKCompat(tf.contrib.learn.DNNRegressor(\n",
        "      model_dir=MODEL_URI, hidden_units=HIDDEN_UNITS, enable_centered_bias=False, feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors_validation_data.values))\n",
        "  )\n",
        "  # check predictions based on defined predictors and assign result to variable\n",
        "  validation_predictions = estimator.predict(x=predictors_validation_data.values)\n",
        "  # extract a list of the resulting prediction scores\n",
        "  validation_prediction_scores = validation_predictions['scores']\n",
        "  # define validation data targets\n",
        "  targets_validation = pd.DataFrame(list(VALIDATION_DATA[t] for t in VALIDATION_TARGETS))\n",
        "  # calculate RMSE for regression model and actual validation collision data\n",
        "  validation_rmse = np.sqrt(np.mean((targets_validation.values - validation_prediction_scores)**2))\n",
        "  # return  results\n",
        "  return validation_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRc0OoVCovgF"
      },
      "source": [
        "# function to generate table of validation results\n",
        "def gen_validation_results_table(validation_result_list):\n",
        "  create_validation_table(validation_result_list)\n",
        "\n",
        "# function to generate table of results\n",
        "def gen_results_table(rmse_result_list):\n",
        "  return create_results_table(rmse_result_list)\n",
        "\n",
        "# function to do validation\n",
        "def do_validation(predictors_combination):\n",
        "  return run_validation(predictors_combination)\n",
        "  \n",
        "# function to loop DNN regressor n times (shuffling data each time). Returns dict of RMSE results.\n",
        "def run_dnn_regressor(predictors_cols):\n",
        "  # define result lists to return\n",
        "  result_list = []\n",
        "  validation_result_list = []\n",
        "  for idx, run in enumerate(range(NUMBER_OF_MODEL_RUNS)):\n",
        "    # shuffle data & print first 5 rows\n",
        "    shuffled = TRAIN_AND_TEST_DATA.iloc[np.random.permutation(len(TRAIN_AND_TEST_DATA))]\n",
        "    # assign predictors to variable\n",
        "    predictors = shuffled.iloc[:,predictors_cols]\n",
        "    # assign targets to variable\n",
        "    targets = shuffled.iloc[:,TARGETS_COLS]\n",
        "    # define training set size - 80% of the shuffled data array length\n",
        "    training_set_size = int(len(shuffled['NUM_COLLISIONS'])*TRAINING_SET_SLICE)\n",
        "    # define test set size - the remaining of the shuffled data array length\n",
        "    test_set_size = len(shuffled['NUM_COLLISIONS']) - training_set_size\n",
        "    # Define number of targets (outputs)\n",
        "    num_of_targets = NUM_TARGETS\n",
        "    # ensure any previously saved training run models are removed\n",
        "    shutil.rmtree(MODEL_URI, ignore_errors=True)\n",
        "    # define the TensorFlow estimator\n",
        "    estimator = tf.contrib.learn.SKCompat(\n",
        "        tf.contrib.learn.DNNRegressor(\n",
        "            model_dir=MODEL_URI, \n",
        "            hidden_units=HIDDEN_UNITS, \n",
        "            optimizer=tf.train.AdamOptimizer(learning_rate=LEARNING_RATE), \n",
        "            enable_centered_bias=False, \n",
        "            feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values)\n",
        "            )\n",
        "        )\n",
        "    # train the model, using previously defined predictor & target values\n",
        "    estimator.fit(predictors[:training_set_size].values, \n",
        "                  targets[:training_set_size].values.reshape(\n",
        "                      training_set_size, num_of_targets)/SCALE_NUM_COLLISIONS, steps=ESTIMATOR_STEPS)\n",
        "    # check predictions based on defined predictors and assign result to variable\n",
        "    predictions = estimator.predict(x=predictors[training_set_size:].values)\n",
        "    # Apply scaling to output (not require here as previously discussed)\n",
        "    predictions_scaled = predictions['scores']*SCALE_NUM_COLLISIONS\n",
        "    # format predictions as strings to make readable in printing to check output etc\n",
        "    predictions_formatted = format(str(predictions_scaled))\n",
        "    # calculate RSME to determine how well the model is predicting the targets from the predictors\n",
        "    rmse_dnn_model = np.sqrt(np.mean((targets[training_set_size:].values - predictions_scaled)**2))\n",
        "    # calculate mean of the normalised collision values\n",
        "    mean_norm_collisions = np.mean(shuffled['NUM_COLLISIONS'][:training_set_size])\n",
        "    # calculate the RMSE of line of best fit that simply follows a mean of target values.\n",
        "    # This allows comparison between the 'mean model' and the proposed DNN regression model.\n",
        "    # If the DNN regression model is successful, it will show a 'better fit' than the 'mean model'\n",
        "    # i.e., the resulting calculation should be *lower*.\n",
        "    rmse_mean_model = np.sqrt(np.mean((shuffled['NUM_COLLISIONS'][training_set_size:] - mean_norm_collisions)**2))\n",
        "    # add rmse calculations to result list\n",
        "    result_list.append({'Run': idx+1, 'DNN Regression Model RMSE': rmse_dnn_model, 'Mean Model RMSE': rmse_mean_model, 'Predictors Columns': predictors_cols})\n",
        "    # perform validation process\n",
        "    validation_result_list.append({'Run': idx+1, 'Validation Test RMSE' : do_validation(predictors_cols), 'Predictors Columns': predictors_cols})\n",
        "  # draw table\n",
        "  model_table_generated = gen_results_table(result_list)\n",
        "  # draw validation table results table only if model table was also drawn\n",
        "  if model_table_generated:\n",
        "    print('Validate for these predictors: {}'.format(\", \".join(f\"\\'{VALIDATION_DATA.columns[c]}\\'\" for c in predictors_cols)))\n",
        "    print('Validate for these targets: {}\\n'.format(\", \".join(f\"\\'{t}\\'\" for t in VALIDATION_TARGETS)))\n",
        "    gen_validation_results_table(validation_result_list)\n",
        "  # return results\n",
        "  return [result_list, validation_result_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mdZ_nx58XSQ"
      },
      "source": [
        "# - run all models n times, for all predictor combinations on imported datasets;\n",
        "# - calculate & tabulate RMSEs for each model run & then calculate mean RMSE;\n",
        "# - run model n time on validation datasets, for all predictor combinations;\n",
        "# - calculate RMSE for each model validation run & then calculate mean RMSE;\n",
        "\n",
        "# function to do modelling & validation\n",
        "def model_and_validate():\n",
        "  # define list to retain all results in memory for further analysis if necessary\n",
        "  all_rmse_results = []\n",
        "  # print sheet header\n",
        "  print(f'Tensor Flow DNN Regression Output')\n",
        "  print(f'Model dataset title: {DATASET_TITLE}')\n",
        "  print(f'Model dataset CSV: {DATA_URL}')\n",
        "  print(f'Model validation dataset CSV: {VALIDATION_DATA_URL}')\n",
        "  print(f'Model Learning Rate: {LEARNING_RATE}')\n",
        "  print(f'Model Hidden Layers: {HIDDEN_UNITS}')\n",
        "  print(f'------------------------------------------------------------------------------\\n')\n",
        "  # print the predictor combination\n",
        "  print('Model for these predictors: {}'.format(\", \".join(f\"\\'{TRAIN_AND_TEST_DATA.columns[c]}\\'\" for c in PREDICTORS_COLS)))\n",
        "  print('Model for these targets: {}\\n'.format(\", \".join(f\"\\'{TRAIN_AND_TEST_DATA.columns[t]}\\'\" for t in TARGETS_COLS)))\n",
        "  results = run_dnn_regressor(PREDICTORS_COLS)\n",
        "  # add results to in-memory list\n",
        "  all_rmse_results.append({'model_results': results[0], 'validation_results': results[1]})\n",
        "  print(f'\\n-------\\n')\n",
        "  # generate rank table of median rmse result over n runs\n",
        "  generate_rank_table(all_rmse_results)\n",
        "  return all_rmse_results\n",
        "\n",
        "# function to generate rank table of results\n",
        "def generate_rank_table(all_results):\n",
        "  create_rank_table(all_results)\n",
        "  \n",
        "all_rmse_results = model_and_validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS78qZe_DKix"
      },
      "source": [
        "# print all results in memory from all runs (optional)\n",
        "print(all_rmse_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}